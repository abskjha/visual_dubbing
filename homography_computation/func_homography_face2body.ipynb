{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version =  3.5.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print('Python Version = ',platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import dlib\n",
    "import face_alignment\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, enable_cuda=True, flip_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid2img(filename):\n",
    "    cap=cv2.VideoCapture(filename)\n",
    "    success,frame=cap.read()\n",
    "    allImg=[]\n",
    "    while(success):\n",
    "        allImg.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        success,frame=cap.read()\n",
    "    return allImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_align(src_img,dst_img,face_alignment_obj=fa):\n",
    "#     img0 = cv2.resize(src_img,(256,256))\n",
    "    img0 = src_img\n",
    "    img1 = dst_img\n",
    "    h,w,d = dst_img.shape\n",
    "    fa_img0 = face_alignment_obj.get_landmarks(img0)[0]\n",
    "    fa_img1 = face_alignment_obj.get_landmarks(img1)[0]\n",
    "    \n",
    "    src_img = img0\n",
    "    src_points = fa_img0[:36, :2]\n",
    "    dst_img = img1\n",
    "    dst_points = fa_img1[:36, :2]\n",
    "#     print(len(dst_points))\n",
    "    \n",
    "    H, status = cv2.findHomography(src_points, dst_points)\n",
    "    warped_img_0_to_1 = cv2.warpPerspective(src_img, H, (w, h))\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.subplot(221)\n",
    "#     plt.imshow(img0)\n",
    "#     plt.subplot(222)\n",
    "#     plt.imshow(warped_img_0_to_1)\n",
    "#     plt.subplot(223)\n",
    "#     plt.imshow(img1)\n",
    "    lm_w_0_to_1 = np.round(face_alignment_obj.get_landmarks(warped_img_0_to_1)[0]).astype('int')\n",
    "    \n",
    "    lip_center_src=np.mean([lm_w_0_to_1[62][:2],lm_w_0_to_1[66][:2]],0)\n",
    "    lc_src=np.round(lip_center_src).astype('int')\n",
    "    lip_center_dst=np.mean([fa_img1[62][:2],fa_img1[66][:2]],0)\n",
    "    lc_dst=np.round(lip_center_dst).astype('int')\n",
    "    buffer_factor=1.1\n",
    "    dist=np.round(np.sqrt(np.sum(np.square(lm_w_0_to_1[33,:2]-lc_src)))*buffer_factor).astype('int')\n",
    "    img_trans=img1\n",
    "    img_trans[lc_dst[1]-dist:lc_dst[1]+dist,lc_dst[0]-dist:lc_dst[0]+dist]= warped_img_0_to_1[lc_src[1]-dist:lc_src[1]+dist,lc_src[0]-dist:lc_src[0]+dist]\n",
    "    return(img_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allImg2vid(allImg, output_vid='output_body.mp4', frameRate=30):\n",
    "    vidCodec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    height, width, channel = allImg[0].shape\n",
    "    vidFile = cv2.VideoWriter(output_vid, vidCodec, frameRate, (width, height))\n",
    "    for i in tqdm.tqdm(range(0,len(allImg))):\n",
    "        frame = cv2.cvtColor(allImg[i],cv2.COLOR_RGB2BGR)\n",
    "        vidFile.write(frame)\n",
    "    vidFile.release()\n",
    "    print(\" Successfully converted images in allImg to \"+output_vid)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homography_video(target_video_fps, making_video_filename, target_video_filename, input_video_filenme, fa=fa):\n",
    "#     keep target_video_fps=30 (default) for Andrew Ng\n",
    "#     making_video_filename has four face sections in same video\n",
    "#     target_video_filename is video with body\n",
    "#     input_video_filename is used for audio transfer\n",
    "    allImg_im0=vid2img(making_video_filename)\n",
    "    allImg_im1=vid2img(target_video_filename)\n",
    "    min_len=min([len(allImg_im0),len(allImg_im1)])\n",
    "    \n",
    "    h, w, d=allImg_im1[0].shape\n",
    "    allImg_trans=np.zeros((min_len, h, w, d)).astype('uint8')\n",
    "    for i in tqdm.tqdm(range(0,min_len)):\n",
    "        allImg_trans[i]=face_align(allImg_im0[i][256:,:256],allImg_im1[i],face_alignment_obj=fa)\n",
    "    \n",
    "    allImg2vid(allImg_trans, frameRate=target_video_fps)\n",
    "    shell_script='ffmpeg -i output_body.mp4 -i '+input_video_filenme+' -c copy -map 0:v:0 -map 1:a:0 -shortest output_with_body.mp4'\n",
    "    os.system(shell_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1107 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1107 [00:01<25:39,  1.39s/it]\u001b[A\n",
      "  0%|          | 2/1107 [00:02<25:29,  1.38s/it]\u001b[A\n",
      "  0%|          | 3/1107 [00:04<25:25,  1.38s/it]\u001b[A\n",
      "  0%|          | 4/1107 [00:05<25:26,  1.38s/it]\u001b[A\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/abhishek/.local/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 31%|███       | 338/1107 [07:47<17:44,  1.38s/it]"
     ]
    }
   ],
   "source": [
    "inputVid='input_videos/CV_01_C4W1L01_000003_to_000045/CV_01_C4W1L01_000003_to_000045_dynamic_lipsync_eng.mp4_hindi_abhishek_making.mp4'\n",
    "outputVid='input_videos/CV_01_C4W1L01_000003_to_000045/CV_01_C4W1L01_000003_to_000045.mp4'\n",
    "\n",
    "\n",
    "get_homography_video(25, inputVid, outputVid, inputVid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputVid='input_videos/CV_01_C4W1L01_000003_to_000045/CV_01_C4W1L01_000003_to_000045_dynamic_lipsync_eng.mp4_hindi_abhishek_making.mp4'\n",
    "shell_script='ffmpeg -i output_body.mp4 -i '+inputVid+' -c copy -map 0:v:0 -map 1:a:0 -shortest output_with_body.mp4'\n",
    "os.system(shell_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
